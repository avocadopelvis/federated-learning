{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSybuY15XP8p48G07sfPXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avocadopelvis/BTP/blob/main/lung-segmentation/fl-ls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r4q3Ztd1Rfju"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_unet_collection"
      ],
      "metadata": {
        "id": "jEGbNnikSJZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef6094a-2cf8-4240-f304-c42e9c2a53ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_unet_collection\n",
            "  Downloading keras_unet_collection-0.1.13-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras-unet-collection\n",
            "Successfully installed keras-unet-collection-0.1.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from cv2 import imread, createCLAHE\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Mdd9plDGSJcM"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset path\n",
        "image_path = os.path.join(\"/content/drive/MyDrive/BTP/LUNG SEGMENTATION/DATASET/CXR_png\")\n",
        "mask_path = os.path.join(\"/content/drive/MyDrive/BTP/LUNG SEGMENTATION/DATASET/masks\")"
      ],
      "metadata": {
        "id": "G12EeNNwSJef"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Since we have 800 images but only 704 masks, we will make a 1-1 correspondence from masks to images"
      ],
      "metadata": {
        "id": "j9RGGVqJSb4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = os.listdir(image_path)\n",
        "mask = os.listdir(mask_path)\n",
        "# get the file name of each mask and store in a list\n",
        "masks = [fName.split(\".png\")[0] for fName in mask]\n",
        "# get the corresponding image file name for each mask and store in a list\n",
        "images = [fName.split(\"_mask\")[0] for fName in masks]"
      ],
      "metadata": {
        "id": "0W3Zs8iISJhM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check = [i for i in masks if \"mask\" in i]\n",
        "print(\"Total mask that has modified name:\", len(check))"
      ],
      "metadata": {
        "id": "5Wl9I6ACSJjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a168a4-f766-4734-d7bc-00961970f8af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total mask that has modified name: 566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_files = set(os.listdir(image_path)) & set(os.listdir(mask_path))\n",
        "training_files = check\n",
        "\n",
        "# function to get data\n",
        "def getData(X_shape, flag = \"test\"):\n",
        "    im_array = []\n",
        "    mask_array = []\n",
        "    \n",
        "    if flag == \"test\":\n",
        "        for i in tqdm(testing_files): \n",
        "            im = cv2.resize(cv2.imread(os.path.join(image_path,i)),(X_shape,X_shape))[:,:,0]\n",
        "            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i)),(X_shape,X_shape))[:,:,0]\n",
        "            \n",
        "            im_array.append(im)\n",
        "            mask_array.append(mask)\n",
        "        \n",
        "        return im_array,mask_array\n",
        "    \n",
        "    if flag == \"train\":\n",
        "        for i in tqdm(training_files): \n",
        "            im = cv2.resize(cv2.imread(os.path.join(image_path,i.split(\"_mask\")[0]+\".png\")),(X_shape,X_shape))[:,:,0]\n",
        "            mask = cv2.resize(cv2.imread(os.path.join(mask_path,i+\".png\")),(X_shape,X_shape))[:,:,0]\n",
        "\n",
        "            im_array.append(im)\n",
        "            mask_array.append(mask)\n",
        "\n",
        "        return im_array,mask_array\n",
        "    \n",
        "    \n",
        "# function to perform sanity check\n",
        "def plotMask(X,y):\n",
        "    sample = []\n",
        "    \n",
        "    for i in range(6):\n",
        "        left = X[i]\n",
        "        right = y[i]\n",
        "        combined = np.hstack((left,right))\n",
        "        sample.append(combined)\n",
        "        \n",
        "        \n",
        "    for i in range(0,6,3):\n",
        "\n",
        "        plt.figure(figsize=(25,10))\n",
        "        \n",
        "        plt.subplot(2,3,1+i)\n",
        "        plt.imshow(sample[i])\n",
        "        \n",
        "        plt.subplot(2,3,2+i)\n",
        "        plt.imshow(sample[i+1])\n",
        "        \n",
        "        \n",
        "        plt.subplot(2,3,3+i)\n",
        "        plt.imshow(sample[i+2])\n",
        "        \n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "DiNamcsvSJmj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dim = 256*2\n",
        "X_train, y_train = getData(dim, flag = 'train')\n",
        "X_test, y_test = getData(dim)"
      ],
      "metadata": {
        "id": "YA7q_ijlSJpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb7ae48-5fbe-4f30-9489-0ce811aa7997"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 566/566 [13:19<00:00,  1.41s/it]\n",
            "100%|██████████| 138/138 [03:31<00:00,  1.53s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine datasets and use it as a unified dataset\n",
        "X_train = np.array(X_train).reshape(len(X_train), dim, dim, 1)\n",
        "y_train = np.array(y_train).reshape(len(y_train), dim, dim, 1)\n",
        "X_test = np.array(X_test).reshape(len(X_test), dim, dim, 1)\n",
        "y_test = np.array(y_test).reshape(len(y_test), dim, dim, 1)\n",
        "\n",
        "assert X_train.shape == y_train.shape\n",
        "assert X_test.shape == y_test.shape\n",
        "\n",
        "images = np.concatenate((X_train, X_test), axis = 0)\n",
        "masks = np.concatenate((y_train, y_test), axis = 0)"
      ],
      "metadata": {
        "id": "aLNml3fvSJrw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train, validation and test sets\n",
        "train_vol, validation_vol, train_seg, validation_seg = train_test_split((images-127.0)/127.0, \n",
        "                                                                        (masks>127).astype(np.float32), \n",
        "                                                                        test_size = 0.2,random_state = 42)\n",
        "\n",
        "train_vol, test_vol, train_seg, test_seg = train_test_split(train_vol,train_seg, \n",
        "                                                            test_size = 0.2, \n",
        "                                                            random_state = 42)"
      ],
      "metadata": {
        "id": "KOemEf77OErO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = 450\n",
        "# client = 450/3 = 150"
      ],
      "metadata": {
        "id": "OawMwjeOYOw8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "# from keras.optimizers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras import backend as keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = keras.flatten(y_true)\n",
        "    y_pred_f = keras.flatten(y_pred)\n",
        "    intersection = keras.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def unet(input_size=(256,256,1)):\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])"
      ],
      "metadata": {
        "id": "gMmhi9nUSJuj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to take in data and return a dictionary with client names as keys and values as data shards\n",
        "def create_client(x, y, num_clients, initial = 'client'):\n",
        "  # create a list of client names\n",
        "  client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "  # size of data shard\n",
        "  size = len(x)//num_clients\n",
        "  # create data shard for each client\n",
        "  x_shards = [x[i:i+size] for i in range(0, len(x), size)]\n",
        "  y_shards = [y[i:i+size] for i in range(0, len(x), size)]\n",
        "\n",
        "  # number of clients must equal number of shards\n",
        "  assert(len(x_shards) == len(client_names))\n",
        "  assert(len(y_shards) == len(client_names))\n",
        "\n",
        "  return {client_names[i] : [x_shards[i], y_shards[i]] for i in range(len(client_names))} \n",
        "\n",
        "\n",
        "def weight_scaling_factor(data, train):\n",
        "    return len(data)/len(train)\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "        \n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "# function to evaluate the model on test data and print the current round and metrics\n",
        "def evaluate_model(data, model, round): \n",
        "  test_generator = DataGenerator(data)\n",
        "  results = model.evaluate(test_generator, batch_size = batch_size, verbose = 1)\n",
        "  loss, accuracy = results[0], results[1]*100\n",
        "  print(f'round: {round} | loss: {loss} | accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "YIr1SLUqgPj4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create clients\n",
        "clients = create_client(train_vol, train_seg, 3)"
      ],
      "metadata": {
        "id": "iTcMNHaki3T5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one = clients['client_1']\n",
        "# x = one[0]\n",
        "# y = one[1]\n",
        "# len(y)"
      ],
      "metadata": {
        "id": "PkGAbbK3mWWp"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "weight_path=\"{}_weights.best.hdf5\".format('cxr_reg')\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = True)\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
        "                                   patience=3, \n",
        "                                   verbose=1, mode='min', min_delta = 0.0001, cooldown=2, min_lr=1e-6)\n",
        "\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
        "\n",
        "callbacks = [checkpoint, early, reduceLROnPlat]"
      ],
      "metadata": {
        "id": "qA7RF4jETBaz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "loss = [dice_coef_loss]\n",
        "metrics = [dice_coef, 'binary_accuracy']\n",
        "optimizer = Adam(learning_rate = 2e-4)\n",
        "rounds = 5"
      ],
      "metadata": {
        "id": "i8lax9bjOkPI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_unet_collection import models\n",
        "\n",
        "# model = models.unet_2d((512, 512, 1), [32, 64, 128, 256, 512], 2)\n",
        "model = unet(input_size = (512, 512, 1))"
      ],
      "metadata": {
        "id": "OzG6QKT-Szh9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize global model\n",
        "global_model = model\n",
        "global_model.compile(\n",
        "        loss = loss,\n",
        "        optimizer = optimizer,\n",
        "        metrics = metrics\n",
        "        )\n",
        "\n",
        "print(\"Begin Training\")\n",
        "# commence global training loop\n",
        "for round in range(1, rounds+1):\n",
        "  print(f'\\nRound: {round}')\n",
        "\n",
        "  # get global model's weights\n",
        "  global_weights = global_model.get_weights()\n",
        "\n",
        "  # initial list to collect local model weights after scaling\n",
        "  scaled_local_weight_list = list()\n",
        "\n",
        "  # get client names\n",
        "  client_names = list(clients.keys())\n",
        "  random.shuffle(client_names)\n",
        "\n",
        "  count = 1\n",
        "  # loop through each client and create new local model\n",
        "  for client in client_names:\n",
        "    print(f'Client {count}')\n",
        "    local_model = model\n",
        "    local_model.compile(\n",
        "        loss = loss,\n",
        "        optimizer = optimizer,\n",
        "        metrics = metrics\n",
        "        )\n",
        "    \n",
        "    #set local model weight to the weight of the global model\n",
        "    local_model.set_weights(global_weights)\n",
        "\n",
        "    # get client data and pass it through a data generator\n",
        "    # data = DataGenerator(clients[client])\n",
        "\n",
        "    # get client data\n",
        "    x = clients[client][0]\n",
        "    y = clients[client][1]\n",
        "\n",
        "    # fit local model with client's data\n",
        "    local_model.fit(x, y, epochs = epochs, verbose = 1, callbacks = callbacks) #steps_per_epoch = len(x), verbose = 1, callbacks = callbacks, validation_data = valid_generator)\n",
        "\n",
        "    # scale the model weights and add to list\n",
        "    scaling_factor = weight_scaling_factor(x, train_vol)\n",
        "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "    \n",
        "    # not adding scaling\n",
        "    scaled_local_weight_list.append(local_model.get_weights())\n",
        "\n",
        "    # clear session to free memory after each communication round\n",
        "    # K.clear_session()\n",
        "\n",
        "    count += 1\n",
        "\n",
        "  #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "  average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "      \n",
        "  #update global model \n",
        "  global_model.set_weights(average_weights)\n",
        "\n",
        "  # evaluate_model(test_ids, global_model, round)\n",
        "\n",
        "print('\\nTraining Done!')"
      ],
      "metadata": {
        "id": "RO1GJiCcNXYI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(x = train_vol,\n",
        "#                     y = train_seg,\n",
        "#                     batch_size = batch_size,\n",
        "#                     epochs = epochs,\n",
        "#                     validation_data =(test_vol, test_seg) ,\n",
        "#                     callbacks = callbacks)"
      ],
      "metadata": {
        "id": "Z40CvvsPTMTO"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}