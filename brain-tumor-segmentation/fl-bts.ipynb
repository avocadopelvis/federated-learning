{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avocadopelvis/BTP/blob/main/brain-tumor-segmentation/fl-bts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VVl5qg_BKhxL"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrQ6i33VEEs",
        "outputId": "dc8f3a15-2fe6-45a6-94fa-7642c737d34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.9.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nilearn) (4.9.1)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.7.3)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->nilearn) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A_4xGJxhNsZv"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "# glob (short for global) is used to return all file paths that match a specific pattern.\n",
        "import glob \n",
        "\n",
        "# PIL adds image processing capabilities to your Python interpreter.\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file.\n",
        "import shutil\n",
        "\n",
        "# skimage is a collection of algorithms for image processing and computer vision.\n",
        "from skimage import data\n",
        "from skimage.util import montage\n",
        "import skimage.transform as skTrans\n",
        "from skimage.transform import rotate\n",
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "# NEURAL IMAGING\n",
        "import nilearn as nl\n",
        "import nibabel as nib # access a multitude of neuroimaging data formats\n",
        "# import nilearn.plotting as nlplt\n",
        "# import gif_your_nifti.core as gif2nif\n",
        "\n",
        "\n",
        "# ML Libraries\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import CSVLogger\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# make numpy printouts easier to read\n",
        "np.set_printoptions(precision = 3, suppress = True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "erTTPBpVNW2w"
      },
      "outputs": [],
      "source": [
        "# dataset path\n",
        "train_data = \"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/DATASET/BraTS 2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
        "valid_data = \"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/DATASET/BraTS 2020/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YTuVKntPNrNP"
      },
      "outputs": [],
      "source": [
        "# list of directories\n",
        "train_val_directories = [f.path for f in os.scandir(train_data) if f.is_dir()]\n",
        "\n",
        "# remove BraTS20_Training_355 since it has ill formatted name for seg.nii file\n",
        "train_val_directories.remove(train_data + 'BraTS20_Training_355')\n",
        "\n",
        "# function to convert list of paths into IDs\n",
        "def pathListIntoIDs(dirList):\n",
        "  x = []\n",
        "  for i in range(0, len(dirList)):\n",
        "    x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
        "  return x\n",
        "\n",
        "ids = pathListIntoIDs(train_val_directories)\n",
        "\n",
        "# split ids into train+test and validation\n",
        "train_test_ids, val_ids = train_test_split(ids, test_size = 0.2, random_state = 42)\n",
        "# split train+test into train and test                                           \n",
        "train_ids, test_ids = train_test_split(train_test_ids, test_size = 0.15, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bvtChsUlUrpp"
      },
      "outputs": [],
      "source": [
        "# define segmentation areas\n",
        "SEGMENT_CLASSES = {\n",
        "    0 : 'NOT TUMOR',\n",
        "    1 : 'NECROTIC/CORE', # or NON-ENHANCING TUMOR CORE\n",
        "    2 : 'EDEMA',\n",
        "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
        "}\n",
        "\n",
        "# ????????\n",
        "# there are 155 slices per volume\n",
        "# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n",
        "VOLUME_SLICES = 100 \n",
        "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
        "IMG_SIZE = 128\n",
        "\n",
        "# override keras sequence DataGenerator class\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    # generates data for Keras\n",
        "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
        "        # Initialization\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # denotes the number of batches per epoch\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # generate one batch of data\n",
        "\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(Batch_ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # updates indexes after each epoch\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, Batch_ids):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # initialization\n",
        "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
        "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
        "\n",
        "        \n",
        "        # Generate data\n",
        "        for c, i in enumerate(Batch_ids):\n",
        "            case_path = os.path.join(train_data, i)\n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
        "            flair = nib.load(data_path).get_fdata()    \n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
        "            ce = nib.load(data_path).get_fdata()\n",
        "            \n",
        "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
        "            seg = nib.load(data_path).get_fdata()\n",
        "        \n",
        "            for j in range(VOLUME_SLICES):\n",
        "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "\n",
        "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
        "                    \n",
        "        # Generate masks\n",
        "        y[y==4] = 3;\n",
        "        mask = tf.one_hot(y, 4);\n",
        "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
        "        return X/np.max(X), Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ydy0jtj3Ol-p"
      },
      "outputs": [],
      "source": [
        "# train_ids size is 249\n",
        "# break train_ids into 3 parts\n",
        "# size = 249/3 = 82"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wy-uTxmjPSHS"
      },
      "outputs": [],
      "source": [
        "# U-NET\n",
        "def build_unet(inputs, ker_init, dropout):\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n",
        "    \n",
        "    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "    \n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n",
        "    \n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n",
        "    \n",
        "    \n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n",
        "    drop5 = Dropout(dropout)(conv5)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n",
        "    \n",
        "    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n",
        "    merge = concatenate([conv1,up], axis = 3)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "    \n",
        "    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n",
        "    \n",
        "    return Model(inputs = inputs, outputs = conv10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LJhvb6Sg2Mx5"
      },
      "outputs": [],
      "source": [
        "# function to take in data and return a dictionary with client names as keys and values as data shards\n",
        "def create_client(data, num_clients, initial = 'client'):\n",
        "  # create a list of client names\n",
        "  client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "  # size of data shard\n",
        "  size = len(data)//num_clients\n",
        "  # create data shard for each client\n",
        "  shards = [data[i:i+size] for i in range(0, len(data), size)]\n",
        "\n",
        "  # number of clients must equal number of shards\n",
        "  assert(len(shards) == len(client_names))\n",
        "\n",
        "  return {client_names[i] : shards[i] for i in range(len(client_names))} \n",
        "\n",
        "def weight_scaling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "        \n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "# function to evaluate the model on test data and print the current round and metrics\n",
        "def evaluate_model(data, model, round): \n",
        "  test_generator = DataGenerator(data)\n",
        "  results = model.evaluate(test_generator, batch_size = batch_size, verbose = 1)\n",
        "  loss, accuracy = results[0], results[1]*100\n",
        "  print(f'round: {round} | loss: {loss} | accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tDnYXEEI1jAC"
      },
      "outputs": [],
      "source": [
        "# create clients\n",
        "clients = create_client(train_ids, 3)\n",
        "valid_generator = DataGenerator(val_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mi-Kg1wfu2tz"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "\n",
        "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
        "loss = \"categorical_crossentropy\",\n",
        "learning_rate = 0.001\n",
        "optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# add callback for training process\n",
        "callbacks = [\n",
        "#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n",
        "#                               patience=2, verbose=1, mode='auto'),\n",
        "      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, \n",
        "                              patience=2, min_lr=0.000001, verbose=1) \n",
        "#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n",
        "#                             verbose=1, save_best_only=True, save_weights_only = True)\n",
        "    ]\n",
        "\n",
        "# number of global epochs\n",
        "rounds = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V9EB0OWX1KxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9df7c3-2790-4511-a145-96ea6f9cf408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round: 0\n",
            "Client 1\n",
            "83/83 [==============================] - 254s 3s/step - loss: 0.2782 - accuracy: 0.9598 - val_loss: 0.0724 - val_accuracy: 0.9831 - lr: 0.0010\n",
            "Client 2\n",
            "83/83 [==============================] - 150s 2s/step - loss: 0.3459 - accuracy: 0.9602 - val_loss: 0.0692 - val_accuracy: 0.9831 - lr: 0.0010\n",
            "Client 3\n",
            "83/83 [==============================] - 146s 2s/step - loss: 0.4687 - accuracy: 0.9575 - val_loss: 0.0875 - val_accuracy: 0.9833 - lr: 0.0010\n",
            "45/45 [==============================] - 61s 1s/step - loss: 3038.6621 - accuracy: 0.9856\n",
            "round: 0 | loss: 3038.662109375 | accuracy: 98.56%\n",
            "\n",
            "Round: 1\n",
            "Client 1\n",
            "83/83 [==============================] - 207s 2s/step - loss: 28.6252 - accuracy: 0.9665 - val_loss: 0.1066 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Client 2\n",
            "83/83 [==============================] - 148s 2s/step - loss: 75.1823 - accuracy: 0.9743 - val_loss: 0.1137 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Client 3\n",
            "83/83 [==============================] - 154s 2s/step - loss: 62.1704 - accuracy: 0.9768 - val_loss: 0.0819 - val_accuracy: 0.9831 - lr: 0.0010\n",
            "45/45 [==============================] - 60s 1s/step - loss: 40194932.0000 - accuracy: 0.9539\n",
            "round: 1 | loss: 40194932.0 | accuracy: 95.39%\n",
            "\n",
            "Round: 2\n",
            "Client 1\n",
            "83/83 [==============================] - 211s 3s/step - loss: 1037794.0000 - accuracy: 0.9759 - val_loss: 2436.6658 - val_accuracy: 0.9822 - lr: 0.0010\n",
            "Client 2\n",
            "83/83 [==============================] - 145s 2s/step - loss: 1679576.8750 - accuracy: 0.9619 - val_loss: 183.5058 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Client 3\n",
            "83/83 [==============================] - 145s 2s/step - loss: 667141.1875 - accuracy: 0.9744 - val_loss: 10.6392 - val_accuracy: 0.9817 - lr: 0.0010\n",
            "45/45 [==============================] - 62s 1s/step - loss: 67855223619584.0000 - accuracy: 0.9818\n",
            "round: 2 | loss: 67855223619584.0 | accuracy: 98.18%\n",
            "\n",
            "Round: 3\n",
            "Client 1\n",
            "83/83 [==============================] - 204s 2s/step - loss: 10517171142656.0000 - accuracy: 0.9765 - val_loss: 2435949568.0000 - val_accuracy: 0.9781 - lr: 0.0010\n",
            "Client 2\n",
            "83/83 [==============================] - 155s 2s/step - loss: 15685164990464.0000 - accuracy: 0.9768 - val_loss: 2737411840.0000 - val_accuracy: 0.9756 - lr: 0.0010\n",
            "Client 3\n",
            "83/83 [==============================] - 152s 2s/step - loss: 4756854538240.0000 - accuracy: 0.9791 - val_loss: 54050578432.0000 - val_accuracy: 0.9821 - lr: 0.0010\n",
            "45/45 [==============================] - 60s 1s/step - loss: 10732996004233084928.0000 - accuracy: 0.9841\n",
            "round: 3 | loss: 1.0732996004233085e+19 | accuracy: 98.41%\n",
            "\n",
            "Round: 4\n",
            "Client 1\n",
            "83/83 [==============================] - 203s 2s/step - loss: nan - accuracy: 0.9835 - val_loss: nan - val_accuracy: 0.9831 - lr: 0.0010\n",
            "Client 2\n",
            "83/83 [==============================] - 153s 2s/step - loss: nan - accuracy: 0.9836 - val_loss: nan - val_accuracy: 0.9831 - lr: 0.0010\n",
            "Client 3\n",
            "83/83 [==============================] - 148s 2s/step - loss: nan - accuracy: 0.9809 - val_loss: nan - val_accuracy: 0.9831 - lr: 0.0010\n",
            "45/45 [==============================] - 62s 1s/step - loss: nan - accuracy: 0.9856\n",
            "round: 4 | loss: nan | accuracy: 98.56%\n"
          ]
        }
      ],
      "source": [
        "# initialize global model\n",
        "global_model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "global_model.compile(\n",
        "        loss = loss,\n",
        "        optimizer = optimizer,\n",
        "        metrics = metrics\n",
        "        )\n",
        "\n",
        "# commence global training loop\n",
        "for round in range(rounds):\n",
        "  print(f'\\nRound: {round}')\n",
        "  # get global model's weights\n",
        "  global_weights = global_model.get_weights()\n",
        "\n",
        "  # initial list to collect local model weights after scaling\n",
        "  scaled_local_weight_list = list()\n",
        "\n",
        "  # get client names\n",
        "  client_names= list(clients.keys())\n",
        "  random.shuffle(client_names)\n",
        "\n",
        "  count = 1\n",
        "  # loop through each client and create new local model\n",
        "  for client in client_names:\n",
        "    print(f'Client {count}')\n",
        "    local_model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "    local_model.compile(\n",
        "        loss = loss,\n",
        "        optimizer = optimizer,\n",
        "        metrics = metrics\n",
        "        )\n",
        "    \n",
        "    #set local model weight to the weight of the global model\n",
        "    local_model.set_weights(global_weights)\n",
        "\n",
        "    # get client data and pass it through a data generator\n",
        "    data = DataGenerator(clients[client])\n",
        "\n",
        "    # fit local model with client's data\n",
        "    local_model.fit(data, epochs = 1, steps_per_epoch = len(data), callbacks = callbacks, validation_data = valid_generator)\n",
        "\n",
        "    # scale the model weights and add to list\n",
        "    # scaling_factor = weight_scaling_factor(clients, client)\n",
        "    # scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "    \n",
        "    # not adding scaling\n",
        "    scaled_local_weight_list.append(local_model.get_weights())\n",
        "\n",
        "    # clear session to free memory after each communication round\n",
        "    K.clear_session()\n",
        "\n",
        "    count += 1\n",
        "\n",
        "  #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "  average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "      \n",
        "  #update global model \n",
        "  global_model.set_weights(average_weights)\n",
        "\n",
        "  evaluate_model(test_ids, global_model, round)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FZghHlXwPTC2"
      },
      "outputs": [],
      "source": [
        "# client_names= list(clients.keys())\n",
        "# for client in client_names:\n",
        "#   print(clients[client])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-k8McR1MBEgv"
      },
      "outputs": [],
      "source": [
        "# client_data = clients['client_1']\n",
        "# data = DataGenerator(client_data)\n",
        "# valid_generator = DataGenerator(val_ids)\n",
        "\n",
        "# local_model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "# local_model.compile(\n",
        "#       loss = loss,\n",
        "#       optimizer = optimizer,\n",
        "#       metrics = metrics\n",
        "#       )\n",
        "\n",
        "# local_model.fit(data, epochs=1, steps_per_epoch=len(data), verbose = 1) # callbacks = callbacks, validation_data = valid_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "epd_PfmT-GKl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xBlqQS6HPTN-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GRBnitKnPTQi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z-5N7KiwPTTK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZbNF_U85PTV5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ey4NUr8hPTYh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K6M7XwlLOHCg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fl-bts.ipynb",
      "provenance": [],
      "mount_file_id": "1JhA-UJrQS77d_SsJKsgOTC9Yl5MSx1Hl",
      "authorship_tag": "ABX9TyO3kycvaQhUh6eCTSbRO8Ao",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}