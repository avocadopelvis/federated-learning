{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bts-load.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcheAPgeudQGDy0ADU49Gd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avocadopelvis/BTP/blob/main/brain-tumor-segmentation/bts-load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "krJNGxyIKwm4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nilearn"
      ],
      "metadata": {
        "id": "mjU6SWPbNKuw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "# glob (short for global) is used to return all file paths that match a specific pattern.\n",
        "import glob \n",
        "\n",
        "# PIL adds image processing capabilities to your Python interpreter.\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "# Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file.\n",
        "import shutil\n",
        "\n",
        "# NEURAL IMAGING\n",
        "import nilearn as nl\n",
        "import nibabel as nib # access a multitude of neuroimaging data formats\n",
        "\n",
        "# ML Libraries\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import CSVLogger\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# make numpy printouts easier to read\n",
        "np.set_printoptions(precision = 3, suppress = True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "aVNlUn2NLLbG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dice loss as defined above for 4 classes\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "    class_num = 4\n",
        "    for i in range(class_num):\n",
        "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
        "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
        "   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n",
        "        if i == 0:\n",
        "            total_loss = loss\n",
        "        else:\n",
        "            total_loss = total_loss + loss\n",
        "    total_loss = total_loss / class_num\n",
        "#    K.print_tensor(total_loss, message=' total dice coef: ')\n",
        "    return total_loss\n",
        "\n",
        "\n",
        " \n",
        "# define per class evaluation of dice coef\n",
        "# inspired by https://github.com/keras-team/keras/issues/9395\n",
        "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
        "\n",
        "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
        "\n",
        "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n",
        "\n",
        "\n",
        "\n",
        "# Computing Precision \n",
        "def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    \n",
        "# Computing Sensitivity      \n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "\n",
        "# Computing Specificity\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "metadata": {
        "id": "GmmFR19kLaoQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define segmentation areas\n",
        "SEGMENT_CLASSES = {\n",
        "    0 : 'NOT TUMOR',\n",
        "    1 : 'NECROTIC/CORE', # or NON-ENHANCING TUMOR CORE\n",
        "    2 : 'EDEMA',\n",
        "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
        "}\n",
        "\n",
        "# ????????\n",
        "# there are 155 slices per volume\n",
        "# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n",
        "VOLUME_SLICES = 100 \n",
        "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
        "IMG_SIZE = 128\n",
        "\n",
        "# override keras sequence DataGenerator class\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    # generates data for Keras\n",
        "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
        "        # Initialization\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # denotes the number of batches per epoch\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # generate one batch of data\n",
        "\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(Batch_ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # updates indexes after each epoch\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, Batch_ids):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # initialization\n",
        "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
        "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
        "\n",
        "        \n",
        "        # Generate data\n",
        "        for c, i in enumerate(Batch_ids):\n",
        "            case_path = os.path.join(train_data, i)\n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
        "            flair = nib.load(data_path).get_fdata()    \n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
        "            ce = nib.load(data_path).get_fdata()\n",
        "            \n",
        "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
        "            seg = nib.load(data_path).get_fdata()\n",
        "        \n",
        "            for j in range(VOLUME_SLICES):\n",
        "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "\n",
        "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
        "                    \n",
        "        # Generate masks\n",
        "        y[y==4] = 3;\n",
        "        mask = tf.one_hot(y, 4);\n",
        "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
        "        return X/np.max(X), Y"
      ],
      "metadata": {
        "id": "3Ol8VJ6TMJ-d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset path\n",
        "train_data = \"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/DATASET/BraTS 2020/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
        "valid_data = \"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/DATASET/BraTS 2020/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/\"\n",
        "\n",
        "# list of directories\n",
        "train_val_directories = [f.path for f in os.scandir(train_data) if f.is_dir()]\n",
        "\n",
        "# remove BraTS20_Training_355 since it has ill formatted name for seg.nii file\n",
        "train_val_directories.remove(train_data + 'BraTS20_Training_355')\n",
        "\n",
        "# function to convert list of paths into IDs\n",
        "def pathListIntoIDs(dirList):\n",
        "  x = []\n",
        "  for i in range(0, len(dirList)):\n",
        "    x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
        "  return x\n",
        "\n",
        "ids = pathListIntoIDs(train_val_directories)\n",
        "\n",
        "# split ids into train+test and validation\n",
        "train_test_ids, val_ids = train_test_split(ids, test_size = 0.2)\n",
        "# split train+test into train and test                                           \n",
        "train_ids, test_ids = train_test_split(train_test_ids, test_size = 0.15)"
      ],
      "metadata": {
        "id": "t9ONhproL3zB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = DataGenerator(test_ids)"
      ],
      "metadata": {
        "id": "_XGyKoLdMaOB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EN1psthSGr5G"
      },
      "outputs": [],
      "source": [
        "# load trained model\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/model.h5\",\n",
        "                                custom_objects = {'accuracy' : tf.keras.metrics.MeanIoU(num_classes = 4),\n",
        "                                                  'dice_coef' : dice_coef,\n",
        "                                                  'precision' : precision,\n",
        "                                                  'sensitivity' : sensitivity,\n",
        "                                                  'specificity' : specificity,\n",
        "                                                  'dice_coef_necrotic' : dice_coef_necrotic,\n",
        "                                                  'dice_coef_edema' : dice_coef_edema,\n",
        "                                                  'dice_coef_enhancing' : dice_coef_enhancing\n",
        "                                                  }, compile = False)\n",
        "\n",
        "# load training history\n",
        "history = pd.read_csv(\"/content/drive/MyDrive/BTP/BRAIN TUMOR SEGMENTATION/training.log\", sep = ',', engine = 'python')\n",
        "\n",
        "acc = history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "\n",
        "epoch = range(len(acc))\n",
        "\n",
        "loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "dice = history['dice_coef']\n",
        "val_dice = history['val_dice_coef']\n",
        "\n",
        "mean_iou = history['mean_io_u']\n",
        "val_mean_iou = history['val_mean_io_u']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add callback for training process\n",
        "csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
        "\n",
        "callbacks = [\n",
        "#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n",
        "#                               patience=2, verbose=1, mode='auto'),\n",
        "      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=2, min_lr=0.000001, verbose=1),\n",
        "#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n",
        "#                             verbose=1, save_best_only=True, save_weights_only = True)\n",
        "        csv_logger\n",
        "    ]"
      ],
      "metadata": {
        "id": "NAAfKd9UM1pf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
        "              metrics = ['accuracy'])\n",
        "              # , tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing])\n",
        "\n",
        "print(\"Evaluate on test data:\")\n",
        "results = model.evaluate(test_generator, batch_size = 100, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uXNXQV0K4H6",
        "outputId": "341e58db-7f43-49b5-e7b2-46a8447b8b10"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data:\n",
            "45/45 [==============================] - 296s 6s/step - loss: 0.0173 - accuracy: 0.9939\n"
          ]
        }
      ]
    }
  ]
}